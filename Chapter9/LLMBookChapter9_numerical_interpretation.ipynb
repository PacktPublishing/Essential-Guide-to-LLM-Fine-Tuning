{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlJrWU73dvjF",
        "outputId": "99372d4e-613b-48cd-edea-e23027d6a973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U pyarrow --quiet\n",
        "!pip install datasets transformers torch numpy seqeval --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Generate synthetic financial data\n",
        "def generate_financial_data(num_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    revenues = np.random.randint(100000, 10000000, num_samples)\n",
        "    expenses = np.random.randint(50000, 9000000, num_samples)\n",
        "    profits = revenues - expenses\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for i in tqdm(range(num_samples)):\n",
        "        financial_text = f\"Revenue: ${revenues[i]}, Expenses: ${expenses[i]}, Profit: ${profits[i]}\"\n",
        "\n",
        "        if profits[i] > 1000000:\n",
        "            interpretation = \"The company is performing exceptionally well with high profits.\"\n",
        "            label = 2\n",
        "        elif profits[i] > 0:\n",
        "            interpretation = \"The company is profitable but there's room for improvement.\"\n",
        "            label = 1\n",
        "        else:\n",
        "            interpretation = \"The company is operating at a loss and needs immediate attention.\"\n",
        "            label = 0\n",
        "\n",
        "        data.append(financial_text + \" \" + interpretation)\n",
        "        labels.append(label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Create a custom dataset\n",
        "class FinancialDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Generate synthetic data\n",
        "texts, labels = generate_financial_data()\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = FinancialDataset(texts, labels, tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Function to interpret new financial data\n",
        "def interpret_financial_data(financial_text):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        financial_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "    interpretations = [\n",
        "        \"The company is operating at a loss and needs immediate attention.\",\n",
        "        \"The company is profitable but there's room for improvement.\",\n",
        "        \"The company is performing exceptionally well with high profits.\"\n",
        "    ]\n",
        "\n",
        "    return interpretations[predicted.item()]\n",
        "\n",
        "# Example usage\n",
        "new_financial_data = \"Revenue: $8500000, Expenses: $7000000, Profit: $1500000\"\n",
        "interpretation = interpret_financial_data(new_financial_data)\n",
        "print(f\"Financial Data: {new_financial_data}\")\n",
        "print(f\"Interpretation: {interpretation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFiRjdXRdx96",
        "outputId": "63f74b3d-01a1-40e9-fa37-f750e90886df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/63 [00:27<27:59, 27.08s/it]\u001b[A\n",
            "  3%|▎         | 2/63 [00:48<23:52, 23.48s/it]\u001b[A\n",
            "  5%|▍         | 3/63 [01:08<22:19, 22.33s/it]\u001b[A\n",
            "  6%|▋         | 4/63 [01:28<20:52, 21.22s/it]\u001b[A\n",
            "  8%|▊         | 5/63 [01:48<19:55, 20.61s/it]\u001b[A\n",
            " 10%|▉         | 6/63 [02:08<19:28, 20.50s/it]\u001b[A\n",
            " 11%|█         | 7/63 [02:27<18:44, 20.08s/it]\u001b[A\n",
            " 13%|█▎        | 8/63 [02:47<18:27, 20.14s/it]\u001b[A\n",
            " 14%|█▍        | 9/63 [03:06<17:49, 19.81s/it]\u001b[A\n",
            " 16%|█▌        | 10/63 [03:27<17:37, 19.95s/it]\u001b[A\n",
            " 17%|█▋        | 11/63 [03:46<17:06, 19.74s/it]\u001b[A\n",
            " 19%|█▉        | 12/63 [04:06<16:52, 19.86s/it]\u001b[A\n",
            " 21%|██        | 13/63 [04:26<16:30, 19.81s/it]\u001b[A\n",
            " 22%|██▏       | 14/63 [04:45<16:07, 19.74s/it]\u001b[A\n",
            " 24%|██▍       | 15/63 [05:06<15:54, 19.89s/it]\u001b[A\n",
            " 25%|██▌       | 16/63 [05:25<15:26, 19.70s/it]\u001b[A\n",
            " 27%|██▋       | 17/63 [05:45<15:15, 19.90s/it]\u001b[A\n",
            " 29%|██▊       | 18/63 [06:04<14:47, 19.71s/it]\u001b[A\n",
            " 30%|███       | 19/63 [06:25<14:34, 19.88s/it]\u001b[A\n",
            " 32%|███▏      | 20/63 [06:44<14:07, 19.72s/it]\u001b[A\n",
            " 33%|███▎      | 21/63 [07:04<13:51, 19.80s/it]\u001b[A\n",
            " 35%|███▍      | 22/63 [07:24<13:34, 19.87s/it]\u001b[A\n",
            " 37%|███▋      | 23/63 [07:43<13:06, 19.65s/it]\u001b[A\n",
            " 38%|███▊      | 24/63 [08:04<12:55, 19.87s/it]\u001b[A\n",
            " 40%|███▉      | 25/63 [08:23<12:27, 19.67s/it]\u001b[A\n",
            " 41%|████▏     | 26/63 [08:43<12:14, 19.84s/it]\u001b[A\n",
            " 43%|████▎     | 27/63 [09:02<11:47, 19.64s/it]\u001b[A\n",
            " 44%|████▍     | 28/63 [09:23<11:35, 19.86s/it]\u001b[A\n",
            " 46%|████▌     | 29/63 [09:42<11:12, 19.77s/it]\u001b[A\n",
            " 48%|████▊     | 30/63 [10:02<10:53, 19.80s/it]\u001b[A\n",
            " 49%|████▉     | 31/63 [10:22<10:38, 19.94s/it]\u001b[A\n",
            " 51%|█████     | 32/63 [10:42<10:12, 19.74s/it]\u001b[A\n",
            " 52%|█████▏    | 33/63 [11:02<09:58, 19.96s/it]\u001b[A\n",
            " 54%|█████▍    | 34/63 [11:21<09:34, 19.79s/it]\u001b[A\n",
            " 56%|█████▌    | 35/63 [11:42<09:17, 19.92s/it]\u001b[A\n",
            " 57%|█████▋    | 36/63 [12:01<08:52, 19.74s/it]\u001b[A\n",
            " 59%|█████▊    | 37/63 [12:21<08:37, 19.90s/it]\u001b[A\n",
            " 60%|██████    | 38/63 [12:42<08:20, 20.04s/it]\u001b[A\n",
            " 62%|██████▏   | 39/63 [13:01<07:54, 19.77s/it]\u001b[A\n",
            " 63%|██████▎   | 40/63 [13:21<07:38, 19.93s/it]\u001b[A\n",
            " 65%|██████▌   | 41/63 [13:40<07:13, 19.70s/it]\u001b[A\n",
            " 67%|██████▋   | 42/63 [14:01<06:57, 19.89s/it]\u001b[A\n",
            " 68%|██████▊   | 43/63 [14:20<06:34, 19.74s/it]\u001b[A\n",
            " 70%|██████▉   | 44/63 [14:40<06:18, 19.90s/it]\u001b[A\n",
            " 71%|███████▏  | 45/63 [15:00<05:57, 19.87s/it]\u001b[A\n",
            " 73%|███████▎  | 46/63 [15:20<05:35, 19.75s/it]\u001b[A\n",
            " 75%|███████▍  | 47/63 [15:40<05:18, 19.92s/it]\u001b[A\n",
            " 76%|███████▌  | 48/63 [15:59<04:55, 19.72s/it]\u001b[A\n",
            " 78%|███████▊  | 49/63 [16:19<04:38, 19.89s/it]\u001b[A\n",
            " 79%|███████▉  | 50/63 [16:38<04:15, 19.63s/it]\u001b[A\n",
            " 81%|████████  | 51/63 [16:59<03:57, 19.80s/it]\u001b[A\n",
            " 83%|████████▎ | 52/63 [17:18<03:36, 19.64s/it]\u001b[A\n",
            " 84%|████████▍ | 53/63 [17:38<03:18, 19.81s/it]\u001b[A\n",
            " 86%|████████▌ | 54/63 [17:58<02:59, 19.89s/it]\u001b[A\n",
            " 87%|████████▋ | 55/63 [18:18<02:37, 19.73s/it]\u001b[A\n",
            " 89%|████████▉ | 56/63 [18:38<02:19, 19.95s/it]\u001b[A\n",
            " 90%|█████████ | 57/63 [18:57<01:58, 19.76s/it]\u001b[A\n",
            " 92%|█████████▏| 58/63 [19:18<01:39, 19.95s/it]\u001b[A\n",
            " 94%|█████████▎| 59/63 [19:37<01:19, 19.77s/it]\u001b[A\n",
            " 95%|█████████▌| 60/63 [19:58<00:59, 19.99s/it]\u001b[A\n",
            " 97%|█████████▋| 61/63 [20:17<00:39, 19.95s/it]\u001b[A\n",
            " 98%|█████████▊| 62/63 [20:37<00:19, 19.87s/it]\u001b[A\n",
            "100%|██████████| 63/63 [20:48<00:00, 19.81s/it]\n",
            " 33%|███▎      | 1/3 [20:48<41:36, 1248.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Average Loss: 0.2660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/63 [00:19<20:01, 19.38s/it]\u001b[A\n",
            "  3%|▎         | 2/63 [00:39<20:13, 19.90s/it]\u001b[A\n",
            "  5%|▍         | 3/63 [01:00<20:09, 20.16s/it]\u001b[A\n",
            "  6%|▋         | 4/63 [01:19<19:31, 19.85s/it]\u001b[A\n",
            "  8%|▊         | 5/63 [01:39<19:24, 20.08s/it]\u001b[A\n",
            " 10%|▉         | 6/63 [01:59<18:51, 19.85s/it]\u001b[A\n",
            " 11%|█         | 7/63 [02:19<18:42, 20.04s/it]\u001b[A\n",
            " 13%|█▎        | 8/63 [02:39<18:09, 19.81s/it]\u001b[A\n",
            " 14%|█▍        | 9/63 [02:59<17:55, 19.92s/it]\u001b[A\n",
            " 16%|█▌        | 10/63 [03:19<17:41, 20.03s/it]\u001b[A\n",
            " 17%|█▋        | 11/63 [03:38<17:11, 19.83s/it]\u001b[A\n",
            " 19%|█▉        | 12/63 [03:59<16:57, 19.94s/it]\u001b[A\n",
            " 21%|██        | 13/63 [04:18<16:27, 19.75s/it]\u001b[A\n",
            " 22%|██▏       | 14/63 [04:38<16:16, 19.93s/it]\u001b[A\n",
            " 24%|██▍       | 15/63 [04:58<15:48, 19.75s/it]\u001b[A\n",
            " 25%|██▌       | 16/63 [05:18<15:38, 19.97s/it]\u001b[A\n",
            " 27%|██▋       | 17/63 [05:38<15:20, 20.01s/it]\u001b[A\n",
            " 29%|██▊       | 18/63 [05:58<14:51, 19.81s/it]\u001b[A\n",
            " 30%|███       | 19/63 [06:18<14:38, 19.97s/it]\u001b[A\n",
            " 32%|███▏      | 20/63 [06:37<14:10, 19.77s/it]\u001b[A\n",
            " 33%|███▎      | 21/63 [06:58<13:57, 19.93s/it]\u001b[A\n",
            " 35%|███▍      | 22/63 [07:17<13:28, 19.72s/it]\u001b[A\n",
            " 37%|███▋      | 23/63 [07:37<13:15, 19.88s/it]\u001b[A\n",
            " 38%|███▊      | 24/63 [07:56<12:50, 19.76s/it]\u001b[A\n",
            " 40%|███▉      | 25/63 [08:16<12:30, 19.76s/it]\u001b[A\n",
            " 41%|████▏     | 26/63 [08:36<12:16, 19.90s/it]\u001b[A\n",
            " 43%|████▎     | 27/63 [08:56<11:49, 19.72s/it]\u001b[A\n",
            " 44%|████▍     | 28/63 [09:16<11:36, 19.90s/it]\u001b[A\n",
            " 46%|████▌     | 29/63 [09:35<11:09, 19.69s/it]\u001b[A\n",
            " 48%|████▊     | 30/63 [09:55<10:54, 19.84s/it]\u001b[A\n",
            " 49%|████▉     | 31/63 [10:15<10:30, 19.69s/it]\u001b[A\n",
            " 51%|█████     | 32/63 [10:35<10:16, 19.88s/it]\u001b[A\n",
            " 52%|█████▏    | 33/63 [10:55<09:56, 19.90s/it]\u001b[A\n",
            " 54%|█████▍    | 34/63 [11:15<09:33, 19.79s/it]\u001b[A\n",
            " 56%|█████▌    | 35/63 [11:35<09:19, 19.98s/it]\u001b[A\n",
            " 57%|█████▋    | 36/63 [11:54<08:54, 19.78s/it]\u001b[A\n",
            " 59%|█████▊    | 37/63 [12:15<08:38, 19.95s/it]\u001b[A\n",
            " 60%|██████    | 38/63 [12:34<08:14, 19.79s/it]\u001b[A\n",
            " 62%|██████▏   | 39/63 [12:55<07:59, 19.99s/it]\u001b[A\n",
            " 63%|██████▎   | 40/63 [13:15<07:40, 20.02s/it]\u001b[A\n",
            " 65%|██████▌   | 41/63 [13:34<07:17, 19.89s/it]\u001b[A\n",
            " 67%|██████▋   | 42/63 [13:55<07:00, 20.03s/it]\u001b[A\n",
            " 68%|██████▊   | 43/63 [14:14<06:36, 19.82s/it]\u001b[A\n",
            " 70%|██████▉   | 44/63 [14:34<06:20, 20.02s/it]\u001b[A\n",
            " 71%|███████▏  | 45/63 [14:54<05:56, 19.81s/it]\u001b[A\n",
            " 73%|███████▎  | 46/63 [15:14<05:39, 19.99s/it]\u001b[A\n",
            " 75%|███████▍  | 47/63 [15:34<05:19, 19.97s/it]\u001b[A\n",
            " 76%|███████▌  | 48/63 [15:54<04:57, 19.83s/it]\u001b[A\n",
            " 78%|███████▊  | 49/63 [16:14<04:40, 20.01s/it]\u001b[A\n",
            " 79%|███████▉  | 50/63 [16:33<04:17, 19.79s/it]\u001b[A\n",
            " 81%|████████  | 51/63 [16:54<03:59, 19.95s/it]\u001b[A\n",
            " 83%|████████▎ | 52/63 [17:13<03:37, 19.78s/it]\u001b[A\n",
            " 84%|████████▍ | 53/63 [17:33<03:19, 19.96s/it]\u001b[A\n",
            " 86%|████████▌ | 54/63 [17:53<02:59, 19.92s/it]\u001b[A\n",
            " 87%|████████▋ | 55/63 [18:13<02:38, 19.82s/it]\u001b[A\n",
            " 89%|████████▉ | 56/63 [18:33<02:20, 20.00s/it]\u001b[A\n",
            " 90%|█████████ | 57/63 [18:53<01:58, 19.80s/it]\u001b[A\n",
            " 92%|█████████▏| 58/63 [19:13<01:39, 19.99s/it]\u001b[A\n",
            " 94%|█████████▎| 59/63 [19:32<01:19, 19.78s/it]\u001b[A\n",
            " 95%|█████████▌| 60/63 [19:53<00:59, 19.95s/it]\u001b[A\n",
            " 97%|█████████▋| 61/63 [20:12<00:39, 19.86s/it]\u001b[A\n",
            " 98%|█████████▊| 62/63 [20:32<00:19, 19.84s/it]\u001b[A\n",
            "100%|██████████| 63/63 [20:43<00:00, 19.73s/it]\n",
            " 67%|██████▋   | 2/3 [41:31<20:45, 1245.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Average Loss: 0.0174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/63 [00:19<19:50, 19.20s/it]\u001b[A\n",
            "  3%|▎         | 2/63 [00:39<20:14, 19.91s/it]\u001b[A\n",
            "  5%|▍         | 3/63 [00:59<20:05, 20.09s/it]\u001b[A\n",
            "  6%|▋         | 4/63 [01:19<19:26, 19.77s/it]\u001b[A\n",
            "  8%|▊         | 5/63 [01:39<19:20, 20.01s/it]\u001b[A\n",
            " 10%|▉         | 6/63 [01:59<18:48, 19.80s/it]\u001b[A\n",
            " 11%|█         | 7/63 [02:19<18:39, 19.98s/it]\u001b[A\n",
            " 13%|█▎        | 8/63 [02:38<18:08, 19.79s/it]\u001b[A\n",
            " 14%|█▍        | 9/63 [02:58<17:56, 19.93s/it]\u001b[A\n",
            " 16%|█▌        | 10/63 [03:19<17:39, 20.00s/it]\u001b[A\n",
            " 17%|█▋        | 11/63 [03:38<17:08, 19.77s/it]\u001b[A\n",
            " 19%|█▉        | 12/63 [03:58<16:57, 19.95s/it]\u001b[A\n",
            " 21%|██        | 13/63 [04:18<16:28, 19.76s/it]\u001b[A\n",
            " 22%|██▏       | 14/63 [04:38<16:19, 19.99s/it]\u001b[A\n",
            " 24%|██▍       | 15/63 [04:58<15:51, 19.83s/it]\u001b[A\n",
            " 25%|██▌       | 16/63 [05:18<15:40, 20.00s/it]\u001b[A\n",
            " 27%|██▋       | 17/63 [05:38<15:25, 20.11s/it]\u001b[A\n",
            " 29%|██▊       | 18/63 [05:58<14:56, 19.93s/it]\u001b[A\n",
            " 30%|███       | 19/63 [06:18<14:43, 20.08s/it]\u001b[A\n",
            " 32%|███▏      | 20/63 [06:38<14:15, 19.90s/it]\u001b[A\n",
            " 33%|███▎      | 21/63 [06:58<14:02, 20.06s/it]\u001b[A\n",
            " 35%|███▍      | 22/63 [07:18<13:35, 19.90s/it]\u001b[A\n",
            " 37%|███▋      | 23/63 [07:38<13:20, 20.01s/it]\u001b[A\n",
            " 38%|███▊      | 24/63 [07:58<13:04, 20.12s/it]\u001b[A\n",
            " 40%|███▉      | 25/63 [08:18<12:35, 19.87s/it]\u001b[A\n",
            " 41%|████▏     | 26/63 [08:38<12:22, 20.06s/it]\u001b[A\n",
            " 43%|████▎     | 27/63 [08:58<11:54, 19.85s/it]\u001b[A\n",
            " 44%|████▍     | 28/63 [09:18<11:40, 20.01s/it]\u001b[A\n",
            " 46%|████▌     | 29/63 [09:37<11:14, 19.83s/it]\u001b[A\n",
            " 48%|████▊     | 30/63 [09:58<10:58, 19.94s/it]\u001b[A\n",
            " 49%|████▉     | 31/63 [10:18<10:41, 20.04s/it]\u001b[A\n",
            " 51%|█████     | 32/63 [10:37<10:15, 19.85s/it]\u001b[A\n",
            " 52%|█████▏    | 33/63 [10:58<10:01, 20.04s/it]\u001b[A\n",
            " 54%|█████▍    | 34/63 [11:17<09:35, 19.83s/it]\u001b[A\n",
            " 56%|█████▌    | 35/63 [11:37<09:20, 20.00s/it]\u001b[A\n",
            " 57%|█████▋    | 36/63 [11:57<08:55, 19.82s/it]\u001b[A\n",
            " 59%|█████▊    | 37/63 [12:17<08:38, 19.95s/it]\u001b[A\n",
            " 60%|██████    | 38/63 [12:37<08:20, 20.04s/it]\u001b[A\n",
            " 62%|██████▏   | 39/63 [12:57<07:56, 19.84s/it]\u001b[A\n",
            " 63%|██████▎   | 40/63 [13:17<07:40, 20.02s/it]\u001b[A\n",
            " 65%|██████▌   | 41/63 [13:37<07:16, 19.83s/it]\u001b[A\n",
            " 67%|██████▋   | 42/63 [13:57<06:59, 19.99s/it]\u001b[A\n",
            " 68%|██████▊   | 43/63 [14:16<06:35, 19.78s/it]\u001b[A\n",
            " 70%|██████▉   | 44/63 [14:37<06:19, 19.97s/it]\u001b[A\n",
            " 71%|███████▏  | 45/63 [14:57<06:00, 20.03s/it]\u001b[A\n",
            " 73%|███████▎  | 46/63 [15:16<05:37, 19.82s/it]\u001b[A\n",
            " 75%|███████▍  | 47/63 [15:37<05:20, 20.02s/it]\u001b[A\n",
            " 76%|███████▌  | 48/63 [15:56<04:57, 19.83s/it]\u001b[A\n",
            " 78%|███████▊  | 49/63 [16:16<04:39, 19.99s/it]\u001b[A\n",
            " 79%|███████▉  | 50/63 [16:36<04:17, 19.79s/it]\u001b[A\n",
            " 81%|████████  | 51/63 [16:56<03:59, 19.97s/it]\u001b[A\n",
            " 83%|████████▎ | 52/63 [17:16<03:39, 19.97s/it]\u001b[A\n",
            " 84%|████████▍ | 53/63 [17:36<03:18, 19.87s/it]\u001b[A\n",
            " 86%|████████▌ | 54/63 [17:56<03:00, 20.03s/it]\u001b[A\n",
            " 87%|████████▋ | 55/63 [18:15<02:38, 19.83s/it]\u001b[A\n",
            " 89%|████████▉ | 56/63 [18:36<02:20, 20.03s/it]\u001b[A\n",
            " 90%|█████████ | 57/63 [18:55<01:59, 19.85s/it]\u001b[A\n",
            " 92%|█████████▏| 58/63 [19:16<01:40, 20.01s/it]\u001b[A\n",
            " 94%|█████████▎| 59/63 [19:36<01:20, 20.04s/it]\u001b[A\n",
            " 95%|█████████▌| 60/63 [19:55<00:59, 19.88s/it]\u001b[A\n",
            " 97%|█████████▋| 61/63 [20:16<00:40, 20.01s/it]\u001b[A\n",
            " 98%|█████████▊| 62/63 [20:35<00:19, 19.81s/it]\u001b[A\n",
            "100%|██████████| 63/63 [20:46<00:00, 19.78s/it]\n",
            "100%|██████████| 3/3 [1:02:17<00:00, 1245.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Average Loss: 0.0062\n",
            "Financial Data: Revenue: $8500000, Expenses: $7000000, Profit: $1500000\n",
            "Interpretation: The company is performing exceptionally well with high profits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from seqeval.metrics import accuracy_score\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define compute metrics function\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Generate synthetic financial data\n",
        "def generate_financial_data(num_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    revenues = np.random.randint(100000, 10000000, num_samples)\n",
        "    expenses = np.random.randint(50000, 9000000, num_samples)\n",
        "    profits = revenues - expenses\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        financial_text = f\"Revenue: ${revenues[i]}, Expenses: ${expenses[i]}, Profit: ${profits[i]}\"\n",
        "\n",
        "        if profits[i] > 1000000:\n",
        "            interpretation = \"The company is performing exceptionally well with high profits.\"\n",
        "            label = 2\n",
        "        elif profits[i] > 0:\n",
        "            interpretation = \"The company is profitable but there's room for improvement.\"\n",
        "            label = 1\n",
        "        else:\n",
        "            interpretation = \"The company is operating at a loss and needs immediate attention.\"\n",
        "            label = 0\n",
        "\n",
        "        data.append(financial_text + \" \" + interpretation)\n",
        "        labels.append(label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Create a custom dataset\n",
        "class FinancialDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Generate synthetic data\n",
        "texts, labels = generate_financial_data()\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FinancialDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
        "val_dataset = FinancialDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Before Training Evaluation results: {eval_results}\")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Post Training Evaluation results: {eval_results}\")\n",
        "\n",
        "# Function to interpret new financial data\n",
        "def interpret_financial_data(financial_text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        financial_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "    interpretations = [\n",
        "        \"The company is operating at a loss and needs immediate attention.\",\n",
        "        \"The company is profitable but there's room for improvement.\",\n",
        "        \"The company is performing exceptionally well with high profits.\"\n",
        "    ]\n",
        "\n",
        "    return interpretations[predicted.item()]\n",
        "\n",
        "# Example usage\n",
        "new_financial_data = \"Revenue: $8500000, Expenses: $7000000, Profit: $1500000\"\n",
        "interpretation = interpret_financial_data(new_financial_data)\n",
        "print(f\"Financial Data: {new_financial_data}\")\n",
        "print(f\"Interpretation: {interpretation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "0lcC7H1kdyGJ",
        "outputId": "75186e62-0d82-4395-e067-69904dbbc977"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 57:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.660900</td>\n",
              "      <td>0.502858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.064867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.008454</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Financial Data: Revenue: $8500000, Expenses: $7000000, Profit: $1500000\n",
            "Interpretation: The company is performing exceptionally well with high profits.\n"
          ]
        }
      ]
    }
  ]
}